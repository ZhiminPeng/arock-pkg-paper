% !TEX root = ./arock_pkg_main.tex
\section{Case study}\label{sec:quick_start}
To illustrate the  usage of \pkg, consider the $\ell_1$ regularized logistic regression problem~\citep{ng2004feature} .
\begin{equation}\label{eq:l1_log}
\Min_{x \in \RR^n} \lambda \|x\|_1 + \sum_{i = 1}^m \log\left(1 + e^{-b_i \cdot a_i^T x}\right),
\end{equation}
where $\{(a_i, b_i)\}_{i = 1}^m, (a_i \in \RR^n, b \in \{1, -1\})$ is the training dataset. For demonstration purposes, we simply set the regularization parameter
$\lambda$ to 1, and the maximum number of epochs to 100, and use \pkg~to solve~\eqref{eq:l1_log} on a machine with 64GB of memory and two Intel Xeon E5-2690 v2 processors (20 cores in total). The following are the \texttt{tmac\_fbs\_l1\_log} commands to solve the model
on the news20 dataset\footnote{ This dataset is from \url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets}. In this case, $m=19,996$ and $n=1,355,191$. } with 1 thread, 4 threads, and 16 threads.
\begin{lstlisting}[language=bash]
# ------------------- running with 1 thread -----------------------#
$ tmac_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 1
[some output skipped]
Computing time  is: 29.53(s).
# ------------------- running with 4 threads ----------------------#
$ tmac_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 4
[some output skipped]
Computing time  is: 11.01(s).
# ------------------- running with 16 threads ---------------------#
$ tmac_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 16
[some output skipped]
Computing time  is: 3.87(s).
\end{lstlisting}
The flags \texttt{-data, -epoch, -nthread, -lambda} are  for the data file, maximum number of epochs,
total number of threads, and regularization parameter $\lambda$ respectively. We can see that the command-line
tool is easy to use. Beyond the simplicity, \pkg~is also efficient in the sense that the solving time is
less than 30 seconds for a problem with more than 1 million variables. We can observe that using 16 threads can achieve approximately 8 times of speedup, reducing the run time to under 4 seconds. Next, we show the major components of the source
codes for building \texttt{tmac\_fbs\_l1\_log}.

We solve \eqref{eq:l1_log} with the forward-backward splitting scheme
\begin{equation}\label{eq:fbs_l1_log}
  x^{k+1} = \underbrace{\underbrace{\prox_{ \eta \lambda \|\cdot\|_1}}_{\text{backward operator}}
  \bigg(\underbrace{x^k - \eta \, \nabla_{x} \big(\sum_{i = 1}^m \log (1 + e^{-b_i \cdot a_i^T x^k}}_{\text{forward operator}})\big)\bigg)}_{\text{forward-backward splitting scheme}},
\end{equation}
where the gradient step of logistic loss and the proximal of $\ell_1$ norm correspond to the
forward operator\footnote{A forward operator computes a (sub)gradient at the current point and takes a negative (sub)gradient step to obtain a new point.} and backward operator\footnote{A backward operator typically solves an optimization problem, and its optimality condition yields a (sub)gradient taken at the new point.} respectively. Algorithm~\ref{alg:fbs_l1_log} shows the details of
implementing~\eqref{eq:fbs_l1_log} in an asynchronous parallel coordinate update fashion.

\begin{algorithm}[H]\label{alg:fbs_l1_log}
\DontPrintSemicolon
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{output}
  \Input{$A, b$ and $x$ are shared variables, $p$ agents, $K > 0$.}
  \textbf{Initialization:} \\
  \quad $\text{foward}(x) := x - \eta \, \nabla_x \,\sum_{i = 1}^m \log (1 + e^{-b_i \cdot a_i^T x})$  \tcp*{forward operator}
  \quad $\text{backward}(x) := \prox_{\eta \lambda  \|\cdot\|_1} (x)$ \tcp*{backward operator}
  \quad $\text{fbs}(x) := \text{backward}(\text{forward}(x))$ \tcp*{foward-backward splitting scheme}
  \quad create $p$ computing agents \\
  % \tcp*{multicore driver}
  \While{each of the $p$ agents continuously}{
    \textbf{selects} $i \in \{1, ..., n\}$ based on some index rule \;
    \textbf{updates} $x_i \gets x_i - \eta \left(x_i - \text{fbs}_i (x)\right)$
  }
  \caption{\pkg~for $\ell_1$ logistic regression.}
\end{algorithm}
The snippet of code (extracted from
\texttt{apps/tmac\_fbs\_l1\_log.cc}) shown in Listing 1 implements Algorithm~\ref{alg:fbs_l1_log} with the \pkg~package.
Specifically, line 3 defines \texttt{forward} as an operator of type \texttt{forward\_grad\_for\_log\_loss<SpMat>}
initialized by the pointers to the data \texttt{A}, which is a sparse matrix, and label \texttt{b}, which is a dense vector. Line 5  defines a
\texttt{prox\_l1} operator (\texttt{backward}) initialized by $\lambda$ and step size $\eta$.
Line 7 defines a forward-backward splitting scheme (\texttt{fbs}) with the previously defined forward operator,
backward operator and the address of the unknown variable \texttt{x}. Line 9 calls the multicore driver
\texttt{TMAC} on the \texttt{fbs} scheme and some user specified parameters (\texttt{params}).
\begin{lstlisting}[caption={example code}\label{code:l1log},language=C++]
  // [...] parameters are defined above
  // forward operator: gradient step for logistic loss
  forward_grad_for_log_loss<SpMat> forward(&A, &b, &Atx, eta);
  // backward operator: proximal operator for l1 norm
  prox_l1 backward(eta, lambda);
  // forward-backward splitting scheme
  ForwardBackwardSplitting<forward_grad_for_log_loss<SpMat>, prox_l1>
    fbs(&x, forward, backward);
  // the multicore driver
  TMAC(fbs, params);
\end{lstlisting}
One can easily adapt the previous code to solve other problems, for example, replacing lines 5 through 7 with
the following two lines
\begin{lstlisting}[ caption={modified line 5-7}\label{code:tikhonovlog}, language=C++]
  prox_sum_square backward(eta, lambda);
  ForwardBackwardSplitting<forward_grad_for_log_loss<SpMat>, prox_sum_square> fbs(&x, forward, backward);
\end{lstlisting}
solves the Tikhonov regularized logistic regression, i.e.,
$$\Min_{x \in \RR^n} \lambda \|x\|_2^2 + \sum_{i = 1}^m \log\left(1 + \exp(-b_i \cdot a_i^T x)\right).$$
Replacing line 3 and line 7 with
the following two lines
\begin{lstlisting}[caption={replaced line 3 and line 7}\label{code:tikhonovlog}, language=C++]
  forward_grad_for_square_loss<Matrix> forward(&A, &b, &Atx, eta);
  ForwardBackwardSplitting<forward_grad_for_square_loss<Matrix>, prox_l1> fbs(&x, forward, backward);
\end{lstlisting}
solves the Lasso problem
$$\Min_{x \in \RR^n} \lambda \|x\|_1 + \frac{1}{2}\|A x - b\|^2.$$
where \texttt{A} is a dense matrix. It is worth mentioning that the previously used operators have implemented in \pkg. Users can refer to the documentation for
the complete list of implemented operators.

