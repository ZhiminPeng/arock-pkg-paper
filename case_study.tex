% !TEX root = ./arock_pkg_main.tex
\section{Case study}\label{sec:quick_start}
To illustrate the  usage of \pkg, consider the $\ell_1$ regularized logistic regression~\citep{ng2004feature}
\begin{equation}\label{eq:l1_log}
\Min_{x \in \RR^n} \lambda \|x\|_1 + \sum_{i = 1}^m \log\left(1 + \exp(-b_i \cdot a_i^T x)\right),
\end{equation}
where $\{(a_i, b_i)\}_{i = 1}^m$ is the training dataset. For demonstration purposes, we simply set the regularization parameter
$\lambda$ to 1, and the maximum number of epochs to 100, and use \pkg~to solve~\eqref{eq:l1_log} on a machine with 64GB of memory and two Intel Xeon E5-2690 v2 processors (20 cores in total). The following are the commands to train the model
on the news20 dataset\footnote{ This dataset is from \url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets}.} with 1 thread, 4 threads, and 16 threads respectively.
\begin{lstlisting}[language=bash]
# ------------------- running with 1 thread -----------------------#
$ arock_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 1
[some output skipped]
Computing time  is: 29.53(s).
# ------------------- running with 4 threads ----------------------#
$ arock_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 4
[some output skipped]
Computing time  is: 11.01(s).
# ------------------- running with 16 threads ---------------------#
$ arock_fbs_l1_log -data news20.svm -epoch 100 -lambda 1 -nthread 16
[some output skipped]
Computing time  is: 3.87(s).
\end{lstlisting}
The \texttt{-data, -epoch, -nthread, -lambda} are the flags \commwy{fix} for the data file, maximum number of epochs,
total number of threads, and regularization parameter $\lambda$ respectively. We can see that the command-line
tool is easy to use. Beyond the simplicity, \pkg~is also efficient in the sense that the training time is
less than 30 seconds for a problem with more than 1 million variables. We can observe that using 16 threads can achieve approximately 8 times of speedup. Next, we show the major components of the source
codes for building \texttt{motac\_fbs\_l1\_log}.

We solve \eqref{eq:l1_log} with the forward-backward splitting scheme
\begin{equation}\label{eq:fbs_l1_log}
  x^{k+1} = \underbrace{\underbrace{\prox_{ \eta \lambda \|\cdot\|_1}}_{\text{backward operator}}
  \bigg(\underbrace{x^k - \eta \, \nabla_{x} \big(\sum_{i = 1}^m \log (1 + \exp(-b_i \cdot a_i^T x^k)}_{\text{forward operator}})\big)\bigg)}_{\text{forward-backward splitting scheme}},
\end{equation}
where the gradient step of logistic loss and the proximal of $\ell_1$ norm correspond to the
forward operator and backward operator respectively. Algorithm~\ref{alg:fbs_l1_log} shows the details of
implementing~\eqref{eq:fbs_l1_log} in an asynchronous parallel coordinate update fashion.

\begin{algorithm}[H]\label{alg:fbs_l1_log}
\DontPrintSemicolon
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{output}
  \Input{$A, b$ and $x$ are shared variables, $p$ agents, $K > 0$.}
  \textbf{Initialization:} \\
  \quad $\text{foward}(x) := x - \eta \, \nabla_x \,\sum_{i = 1}^m \log (1 + \exp(-b_i \cdot a_i^T x))$  \tcp*{forward operator}
  \quad $\text{backward}(x) := \prox_{\eta \lambda  \|\cdot\|_1} (x)$ \tcp*{backward operator}
  \quad $\text{fbs}(x) := \text{backward}(\text{forward}(x))$ \tcp*{foward-backward splitting scheme}
  \quad create $p$ computing agents \\
  % \tcp*{multicore driver}
  \While{each of the $p$ agents continuously}{
    \textbf{selects} $i \in \{1, ..., n\}$ based on some index rule \;
    \textbf{updates} $x_i \gets x_i - \eta \left(x_i - \text{fbs}_i (x)\right)$
  }
  \caption{\pkg~for sparse logistic regression.}
\end{algorithm}
The snippet of code (extracted from
\texttt{apps/motac\_fbs\_l1\_log.cc}) implements Algorithm~\ref{alg:fbs_l1_log} with the \pkg~package.
Specifically, line 3 defines \texttt{forward} as an operator of type \texttt{forward\_grad\_for\_log\_loss<SpMat>}
initialized by the pointers to the data \texttt{A} and label \texttt{b}. Line 5  initializes a
\texttt{prox\_l1} operator (\texttt{backward}) with regularization parameter $\lambda$ and step size $\eta$. \commzp{use consistent language}
Line 7 defines a forward-backward splitting scheme (\texttt{fbs}) with the previously defined forward operator,
backward operator and the address of the unknown variable \texttt{x}. Line 9 calls the driver
\texttt{MOTAC} on the \texttt{fbs} scheme and some user specified parameters (\texttt{params}). One can see
that creating an async-parallel solver can be easily achieved through assembling appropriate operators together.
\begin{lstlisting}[caption={example code}\label{code:l1log},language=C++]
  // [...] parameters are defined above
  // forward operator: gradient step for logistic loss
  forward_grad_for_log_loss<SpMat> forward(&A, &b, &Atx, eta);
  // backward operator: proximal operator for l1 norm
  prox_l1 backward(eta, lambda);
  // forward-backward splitting scheme
  ForwardBackwardSplitting<forward_grad_for_log_loss<SpMat>, prox_l1>
    fbs(&x, forward, backward);
  // the multicore driver
  MOTAC(fbs, params);
\end{lstlisting}
One can also easily adapt the previous code to solve other problems, for example, replacing lines 5 through 7 with
the following two lines
\begin{lstlisting}[language=C++]
  prox_sum_square backward(eta, lambda);
  ForwardBackwardSplitting<forward_grad_for_log_loss<SpMat>, prox_sum_square> fbs(&x, forward, backward);
\end{lstlisting}
solves the Tikhonov regularized logistic regression, i.e.,
$$\Min_{x \in \RR^n} \lambda \|x\|_2^2 + \sum_{i = 1}^m \log\left(1 + \exp(-b_i \cdot a_i^T x)\right).$$
Replacing line 3 and line 7 with
the following two lines
\begin{lstlisting}[language=C++]
  forward_grad_for_square_loss<SpMat> forward(&A, &b, &Atx, eta);
  ForwardBackwardSplitting<forward_grad_for_square_loss<SpMat>, prox_l1> fbs(&x, forward, backward);
\end{lstlisting}
solves the Lasso problem
$$\Min_{x \in \RR^n} \lambda \|x\|_1 + \frac{1}{2}\|A x - b\|^2.$$
It is worth mentioning that the previously used operators are implemented in \pkg. Users can refer to the documentation for
the complete list of implemented operators.

