% !TEX root = ./arock_pkg_main.tex
\section{Introduction}
\pkg~is a toolbox for optimization that implements algorithms based on a set of modern methods for large-scale optimization. The toolbox covers a variety of optimization problems, which can  be both smooth and nonsmooth, convex and nonconvex, as well as constrained and unconstrained.  \pkg~is designed for fast prototyping of scalable algorithms, which can be  single-threaded or multi-threaded, and the multi-threaded code can run either synchronously or asynchronously.

Specifically, \pkg~implements  algorithms based on  the following methods:
\begin{itemize}
\item \textbf{Operator splitting}: a collection of methods that decompose problems in simple subproblems. The original problem often takes the following forms: (i) minimizing $f_1(x)+\cdots+f_n(x)$, (ii) $0\in T_1(x)+\cdots +T_n(x)$, and (iii) minimizing $f_1(x_1)+\cdots+f_n(x_n)$ subject to linear constraints $A_1 x_1+\cdots A_m x_m=b$. In addition, any function $f_i$ can compose with a linear operator, e.g. $f(x) = g(Ax)$.

\item \textbf{Coordinate update}: a collection of methods that find a solution $x$ by updating one, or a few, of its elements each time. The coordinates can be selected according to the random, cyclic, shuffled cyclic, and greedy rules.

\item \textbf{Parallel} % numerical linear algebra procedures or 
coordinate updates (either synchronous or asynchronous).
%\item  \textbf{Random} coordinate updates.
\end{itemize}
These methods are reviewed in Section~\ref{sec:review} below.

\pkg~is not a modeling language but an algorithm development toolbox. %does not reduce a problem automatically into these algorithms.
We demonstrate the usage of the toolbox in the following examples:
 linear system of equations;
quadratic programming;
empirical risk minimization (e.g. $\ell_1$ and $\ell_2$ regularized  regression);
support vector machine;
portfolio optimization;
and nonnegative matrix factorization. \pkg~can use multiple cores efficiently to solve these  problems because it exploits the underlying structures of the problems. % to obtain simple subproblems. %, either through operator splitting or coordinate update, or both, and, furthermore, most of the operations can be performed in parallel and some are themselves easy to parallelize. Therefore, their numerical solutions of these problems can utilize all the cores available.

\subsection{Coding and design}
\pkg~leverages the \cpp~standard and object oriented design, striking for efficiency, portability, and code readability. The package is written in C++ as Matlab does not currently support shared memory programming. The thread library, a new feature of the C++11 standard, provides multithreading that is invariant to operating system. \pkg~can be compiled by standard C++ compilers  under Linux, Mac, and Windows. We design the package so that the user can implement a sophisticated operator-splitting, coordinate-update, and sequential or parallel algorithm  with little effort. Our codebase is separated into layers: executables, multicore drivers, schemes, operators, and numerical linear algebra\footnote{For the best performance, BLAS is called for numerical algebra operations (e.g., the product of a matrix and a vector plus another vector). While \pkg~can parallelize coordinate updates, it is also possible to parallelize numerical algebra operations by linking \pkg~with a parallel BLAS package such as ScaLAPACK\citep{blackford1997scalapack}.}  that correspond to different algorithmic components. \pkg~is used by combining objects from each layer.
Therefore, our codes  are short, clean, and thus easy to read and modify.

%We design the package so that the user can implement a sophisticated operator-splitting, coordinate-update, and sequential or parallel algorithm  with little effort. The user need only  specify a function, or an operator, either from a provided list or using their own implementation. A guide to implementation is provided. The implemented operator-splitting layers provides a variety of coordinate update mappings that utilize ``forward" and ``backward" operators such as gradient descent and proximal mappings of a function, respectively. The  drivers execute the coordinate updates. (asynchronous) parallelism can be turned on by simply specifying the number of threads to use.
%The above functionalities are supported by our novel modular architecture.

%of  multilevel approach which reduces the gap between expert to low-level programming and novice-level programming.
%The library reduces the barrier to entry on doing async-parallel computing. Users of ARock can either use the prebuilt solvers or build their own async-parallel solvers by simply knowing a few concepts of classic optimization formulations and algorithms.

% \pkg supports a wide range of prebuilt applications, including, but not limited to,
%linear equations, $\ell_1$ and $\ell_2$ regularized logistic regression, portfolio optimization,
%Lasso, ridge regression, robust regression, quadratic programming, and nonnegative matrix factorization.

\subsection{Download and installation}
The \pkg~package can be access from GitHub at~\repo. The package runs on Linux, Mac, and Windows operating systems.
\subsection{Literature}\label{sec:review}
\subsection*{Operator splitting methods:}  These methods solve complicated optimization and monotone inclusion problems by simple subproblems. They started to appear in 1950s for  solving partial differential equations and feasibility problems and were rapidly developed during the 1960s--1980s. Several splitting methods such as Forward-Backward \citep{Passty1979_ergodic}, Douglas-Rachford \citep{DR56} (which is equivalent to ADMM \citep{GabayMercier1976_dual,GlowinskiMarroco1975_approximation}), and Peaceman-Rachford \citep{PR} were introduced. Recently,
operator splitting methods such as ADMM and Split Bregman \citep{GoldsteinOsher2009_split} (also see \citep{WangYangYinZhang2008_NewAlternating}) have found new applications in image processing,
statistical and machine learning, compressive sensing, and control. New methods such as primal-dual splitting \citep{Condat2013_primaldual,Vu2013_splitting}, three-operator splitting \citep{DavisYin2015_threeoperator}, and other primal-dual splitting methods \citep{ChenHuangZhang2016_primaldual,ChenHuangZhang2016_primalduala,LiShenXuZhang2015_multistep} have appeared, and they are designed to solve more complicated problems.

%Software packages based on operator splitting  .... include TFOCS \citep{BeckerCandesGrant2011_TemplatesConvex} .....

\subsection*{Coordinate update methods:}
As the name suggests, these methods update the selected one, or a few, elements of the variable at each iteration. The original  coordinate update method \citep{Hildreth1957_quadratic,Warga1963_minimizing,SargentSebastian1973_convergence,LuoTseng1992_convergence} developed in 1950s and analyzed since then only minimizes the original objective function with respect to the selected coordinates. The later developments such as \citep{GrippoSciandrone2000_convergence,TsengYun2009_coordinate,TsengYun2009_blockcoordinate,XuYin2013_block,BolteSabachTeboulle2014_proximal} have allowed to minimize other functions that are often easier or more efficient. Lately, coordinate descent has been extended so that each update no longer minimizes a function, but instead the update is based on an operator, such as the coordinate projection of an operator or a coordinate-wise fixed-point to an operator \citep{CombettesPesquet2015_stochastic,BianchiHachemFranck2014_stochastic,PengXuYanYin2015_arock,PengWuXuYanYin2016_coordinate}. Hence, we call it coordinate update instead of coordinate descent.

The initial coordinate selection rule  is cyclic selection. It was widely used before other rules such as random \citep{Nesterov2012_efficiency,RichtarikTakac2014_iteration,LuXiao2015_complexity}, shuffled cyclic, greedy \citep{BertsekasBertsekas1999_nonlinear,LiOsher2009_coordinate,TsengYun2009_coordinate,PengYanYin2013_parallel,NutiniSchmidtLaradjiFriedlanderKoepke2015_coordinate}, and parallel \citep{BradleyKyrolaBicksonGuestrin2011_parallel,richtarik2016parallel} started to appear and gain popularity.


\subsection*{Asynchronous parallel methods:}

In  parallel algorithms, the multiple threads need to communicate. An algorithm is synchronous if all the  threads must finish computing before they exchange data, and only after the exchange is done can they start the next computing cycle. When the threads run on multiple cores, synchronization requires every core to wait for the slowest core
(or the one solving the most difficult subproblem) to be communicated,  leading to so-called  bus contention. The threads in an asynchronous parallel algorithm, however, can run continuously; they can compute with whatever information they have, even if the latest information from other agents
have not arrived; they write their results to the shared memory while other threads are still computing. They do not wait for each other. Async-parallel can be traced back to~\citep{chazan1969chaotic} for solving systems of linear equations.
For function minimization,~\citep{bertsekas1989parallel} introduced an async-parallel gradient projection method. Convergence rates are obtained in~\citep{tseng1991rate-asyn}.

For fixed-point problems, async-parallel methods date back to~\citep{baudet1978asynchronous}. In  the pre-2010 methods \citep{BMR1997asyn-multisplit,bertsekas1983distributed,Baz200591,el1998flexible} and the review~\citep{frommer2000asynchronous}, each agent updates its own subset of coordinates. Convergence is established under the \emph{$P$-contraction} condition and its variants~\citep{bertsekas1983distributed}. Recently, the works \citep{nedic2001distributed,recht2011hogwild,liu2013asynchronous,liu2014asynchronous,hsieh2015passcode} introduced async-parallel stochastic methods for function minimization.
For fixed-point problems,~\citep{PengXuYanYin2015_arock} introduced async-parallel stochastic methods (ARock), as well as several applications in optimization.


%Synchronous parallel algorithms are easy to control and are efficient if the cores have balanced loads; asynchronous  ones are difficult to control and, if properly controlled, are faster and insensible to load imbalance.

\subsection*{Software packages:}
There exists several packages for solving optimization problems. 
TOFCS~\citep{becker2011templates} is a framework for solving convex cone problems with first order method. 
SCS \citep{ocpb:16} is a convex cone program solver by applying operator splitting method to solve an equivalent
 feasibility problem. 
Epsilon \citep{wytock2015convex} is a package for solving general convex programming by using fast linear and proximal operators.
Though these packages solve convex optimization problems from various applications, they are sequential and do not take advantage of
the multicore systems.  
PASSCoDe \citep{hsieh2015passcode} implements the muticore parallel dual coordinate descent method that solves $\ell_2$ regularized 
empirical risk minimization problems.
APPROX\citep{fercoq2015accelerated} implements  parallel coordinate descent, stochastic dual ascent, and accelerated gradient descent for block seperable objective functions.
AC-DC\citep{richtarik2016parallel} contains a suite a serial, parallel, and distributed coordinate descent algorithms for LASSO, Elastic Net, SVM, and sparse SVM.

%\subsection{Old}
%Coordinate update (CU) methods reduce a large problem to smaller subproblems and are useful for solving large-sized problems.
%These methods handle both linear and nonlinear maps, smooth and nonsmooth functions, and convex and nonconvex problems.
%Though CU methods have different settings and convergence properties,  the monotone operator theory can unify them into a single abstract framework.
%Specifically, CU method can be reduced to the coordinate update of a fixed-point iteration.
%
%
%Existing CU solvers \citep{hsieh2015passcode,jaggi2014communication,recht2011hogwild} either focus on very particular problems or do not scale to large-sized problems due to their sequential implementation.
%Adapting them to solve slightly different large-sized problems requires significant human effort.
%It is of interest to investigate a method for leveraging their common aspects CU methods applied to different problems, and simplify the implementation process for any new applications. (highlight we cover a broader range of problems).
%
%(We provide an architecture amendable for prototyping new solvers.)
%
%(We implemented in C++ with the new C++11 standard for its agonostic to different plateforms. Mention that the naming and readability are nice, interface with BLAS therefore, we have the best performance of linear algebra functions)
%
%In this paper, we present ARock, a C++ library to simplify the implementation of both sequential and parallel coordinate update algorithms. It is a realization of the CF framework~\citep{peng2016coordinate} and the async-parallel framework~\citep{peng2015arock}. Parallelism of ARock is empowered by the thread library from the \texttt{C++11} standard. The novelty of ARock is the introduction of a multilevel approach which reduces the gap between expert to low-level programming and novice-level programming.
%The library reduces the barrier to entry on doing async-parallel computing.
%Users of ARock can either use the prebuilt solvers or build their own async-parallel solvers by simply knowing a few concepts of classic optimization formulations and algorithms.
%
%
%The solvers of these applications can be interacted through easy-to-use command-line tools.
%Other features of ARock include a rich set of operators, comprehensive documentation, and easy-to-use library calls.
%
%
%
%(outline of the paper)
