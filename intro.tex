% !TEX root = ./arock_pkg_main.tex
\section{Introduction}
(a list of problems we can solve)
\pkg~is a toolbox for optimization that implements algorithms based on a set of modern methods for large-scale optimization. It is designed for fast prototyping of scalable algorithms. The toolbox covers a variety of optimization problems, which can  be both smooth and nonsmooth, convex and nonconvex, as well as constrained and unconstrained.  The algorithms solving these problems can be both single-threaded and multi-threaded, and the multi-threaded parallelism can be both synchronous (the standard kind) and asynchronous.  

Specifically, it implements  algorithms based on one or more of the following methods:
\begin{itemize}
\item \textbf{Operator splitting}: a collection of methods that decompose problems in the following form: (i) minimizing $f_1(x)+\cdots+f_n(x)$, (ii) $0\in T_1(x)+\cdots +T_n(x)$, and (iii) minimizing $f_1(x_1)+\cdots+f_n(x_n)$ subject to linear constraints $A_1 x_1+\cdots A_m x_m=b$. In addition, any function $f_i$ can compose with a linear operator, e.g. $f(x) = g(Ax)$.

\item \textbf{Coordinate (descent) update}: a collection of methods that find a solution $x$ by updating one, or a few, of its elements each time.
\item \textbf{Parallelization} of low-level numerical linear algebra and coordinate updates;
\item  \textbf{Randomization} of coordinate updates, either single-threaded or paralle.
\end{itemize}
These methods are reviewed in \S?? below.

\pkg~does not reduce a problem automatically into these algorithms. However, we demonstrate this by  providing many examples. Specifically, the first version of the toolbox includes the following examples:
\begin{itemize}
\item Linear system of equations;
\item Quadratic programming;
\item $\ell_1$ and $\ell_2$ regularized  regression / empirical risk minimization;
\item Support vector machine;
\item Portfolio optimization;
\item Nonnegative matrix factorization.
\end{itemize}
All these  problems can all be solved by algorithms with very simple operations, either through operator splitting or coordinate update, or both, and, furthermore, most of the operations can be performed in parallel and some are themselves easy to parallelize. Therefore, their numerical solutions of these problems can utilize all the cores available.

\subsection{Coding and design}
\pkg~leverages the recent advances in parallel coding, striking for both efficiency and code readability. The package was written under the C++11 standard, which implements multi-core parallelism without any external package. It can be compiled by  recent versions of standard C++ compilers  under Linux, Mac, and Windows. Compared to the traditional C and C++ codes, our codes  are shorter, cleaner, and thus easier to read and modify. 

For the best performance, BLAS is called for low-level numerical algebra operations. It is possible to link with a parallel BLAS package such as ... 

We design the package so that the user can implement a sophisticated operator-splitting, coordinate-update, or (asynchronous) parallel algorithm  with little effort. The user only needs to specify a function, or an operator, from a provided list, or plugging their own ``modules." The operator-splitting driver in the package takes standard ``forward" and ``backward" operators, or gradient descent and proximal maps for functions. The coordinate-update driver also accepts these  operators once its coordinate map is specified, and (asynchronous) parallelism can be turned on by simply specifying the number of threads.
The above functionalities are supported by a  novel modular architecture, which divides the package into several modules that are seamlessly connected.   

%of  multilevel approach which reduces the gap between expert to low-level programming and novice-level programming.
%The library reduces the barrier to entry on doing async-parallel computing. Users of ARock can either use the prebuilt solvers or build their own async-parallel solvers by simply knowing a few concepts of classic optimization formulations and algorithms.
 
% \pkg supports a wide range of prebuilt applications, including, but not limited to, 
%linear equations, $\ell_1$ and $\ell_2$ regularized logistic regression, portfolio optimization, 
%Lasso, ridge regression, robust regression, quadratic programming, and nonnegative matrix factorization.

\subsection{Download and installation}
The \pkg~package is maintained on GitHub at \url{http://}, which provides instructions to install on Linux, Mac, and Windows operating systems.  \commwy{Zhimin: I suggest a uclaopt address. Okay with you?}

\subsection{Literature}
\subsection*{Operator splitting methods:}  These solve complicated optimization and monotone inclusion problems. They started to appear in 1950s for solving  solving partial differential equations and feasibility problems and were rapidly developed during the 1960s-1980s. Several splitting methods such as Forward-Backward, Douglas-Rachford  (which is equivalent to ADMM \cite{?,?}), and Peaceman-Rachford were introduced. Recently,  Recently,
operator splitting methods such as ADMM and Split Bregman have found new applications in image processing,
statistical and machine learning, compressive sensing, and control. New methods such as primal-dual splitting, three-operator splitting, and ... have appeared to solve more complicated problems.

General software packages that .... include TFOCS .....

\subsection*{Coordinate update methods:}
These methods update the selected one, or a few, coordinates of the variable at each iteration.  While the original  coordinate update method developed in 1950s only minimizes the original objective function with respect to the selected coordinate, the later developments have allowed to minimize an alternative objective function that is often easier to minimize. Lately, coordinate descent has been extended to coordinate update in the sense that each update no longer corresponds to minimizing an objective function; the update can be the coordinate projection of an operator or a coordinate-wise solution to a fixed-point problem.

The coordinate selection rule also has been evolving. Initially, the  cyclic selection was widely used. Then, other rules have appeared: shuffled cyclic, greedy, parallel, and randomized. (one or two more sentences.)

Coordinate descent for function minimization has been implemented in the following software packages: 

\subsection*{Asynchronous parallel methods:} 


\subsection{Old}
Coordinate update (CU) methods reduce a large problem to smaller subproblems and are useful for solving large-sized problems.
These methods handle both linear and nonlinear maps, smooth and nonsmooth functions, and convex and nonconvex problems. 
Though CU methods have different settings and convergence properties,  the monotone operator theory can unify them into a single abstract framework. 
Specifically, CU method can be reduced to the coordinate update of a fixed-point iteration. 


Existing CU solvers \citep{hsieh2015passcode,jaggi2014communication,recht2011hogwild} either focus on very particular problems or do not scale to large-sized problems due to their sequential implementation. 
Adapting them to solve slightly different large-sized problems requires significant human effort. 
It is of interest to investigate a method for leveraging their common aspects CU methods applied to different problems, and simplify the implementation process for any new applications. (highlight we cover a broader range of problems).

(We provide an architecture amendable for prototyping new solvers.)

(We implemented in C++ with the new C++11 standard for its agonostic to different plateforms. Mention that the naming and readability are nice, interface with BLAS therefore, we have the best performance of linear algebra functions) 

In this paper, we present ARock, a C++ library to simplify the implementation of both sequential and parallel coordinate update algorithms. It is a realization of the CF framework~\citep{peng2016coordinate} and the async-parallel framework~\citep{peng2015arock}. Parallelism of ARock is empowered by the thread library from the \texttt{C++11} standard. The novelty of ARock is the introduction of a multilevel approach which reduces the gap between expert to low-level programming and novice-level programming.
The library reduces the barrier to entry on doing async-parallel computing. 
Users of ARock can either use the prebuilt solvers or build their own async-parallel solvers by simply knowing a few concepts of classic optimization formulations and algorithms.

 
The solvers of these applications can be interacted through easy-to-use command-line tools. 
Other features of ARock include a rich set of operators, comprehensive documentation, and easy-to-use library calls.



(outline of the paper)
