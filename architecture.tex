% !TEX root = ./arock_pkg_main.tex
\section{Architecture}


Writing efficient code is very different from writing an optimization algorithm.
Our toolbox's architecture is designed to mimic how a scientist writes down an optimization algorithm.
The toolbox achieves this by separating into the following layers: Numerical Linear Algebra, Operator, Scheme, Kernel, and Multicore Driver.
Each layer represents a different mathematical component of a parallel optimization algorithm.


The following is a brief description of each layer and how it interacts with the layers above and below it. \commzp{add the picture here}

\subsection{Numerical Linear Algebra}

We use Eigen, Sparse BLAS, and BLAS in our Toolbox. \commzp{add reference}
Directly using efficient numerical packages like BLAS can be intimidating due to complex function calls. We provide simplified function calls for common linear algebra operations like $a_i^T x$ in Algorithm \ref{alg:fbs_l1_log}. This layer insulates the user from the grit of raw numerical implementation. Higher layers use the Numerical Linear Algebra Layer in their implementations.


If our provided functions are not sufficient, documentation for eigen, Sparse BLAS, and BLAS can be found in the following locations. \commzp{documentation for these packages}

\subsection{Operator}

The Operator Layer contains Forward Operator objects (e.g., gradient descent step, subgradient step) and Backward Operator (e.g., proximal mapping, projection) objects. These operators types see heavy reuse throughtout optimization.
For instance, Algorithm \ref{alg:fbs_l1_log}, along with any other gradient based method for sparse logistic regression, requires the computation of the Forward Operator $x - \eta \, \nabla_x \,(\sum_{i = 1}^m \log (1 + \exp(-b_i \cdot a_i^T x)))$. On a similar vein, Nonnegative Matrix Factorization and Nonnegative Least Squares share a backwad operator, the projection onto the positive orthant.

Much as the Numerical Linear Algebra Layer insulates the user from the computation details of numerical linear algebra, the Operator Layer insulates the user from the computational details of operators. This is achieved by encapsulating the computation of common Forward and Backward Operators into Operator objects. \commwy{link back to code snippet, motivate readability, reusability, and fast coding} The Operator Layer, as a result, allows users to reason and code at the level of operators. The higher layers uses Operator objects as components in the creation of algorithms. \commzp{add footnote to explain the
differences between operator object and operator}

As our Toolbox is designed for coordinate update methods, each operator is implemented to compute coordinates efficiently. A coordinate or block of coordinates can be computed efficienty if the computational cost of a single coordinate or a block of coordinates of the operator is reduced by a dimensional factor compared to the evaluation of the entire operator (e.g. $(Ax)_i$ versus $Ax$). In some cases caching --- the storing an intermediate computation --- can improve the efficiency of updating a coordinate block. These ideas are formalized in CFU paper.

In the case that an operator is needed that we have not provided, a user can use CFU paper as a guide to identify coordinate-friendly structures and implement their own operator. In addition, there are certain rules about operator implementation that must be followed; see Section \ref{sc:implement}.


\subsection{Scheme}
A scheme describes how to make a single-iteration update to $x$.
It can be written as a combination of operators. For example, \eqref{eq:fbs_l1_log} is the Forward-Backward Splitting Scheme (also referred as Proximal Gradient Method) for a specific problem, sparse logistic regression. In Algorithm \ref{alg:fbs_l1_log}, it corresponds to Line 9. To apply the Forward-Backward Splitting Scheme to the sparse logistic regression problem \eqref{}, we need to specify a Forward Operator and a Backward Operator (e.g., on Line 5 of Algorithm \ref{alg:fbs_l1_log}). We can see this in code snippet above {\color{red} Need a way to reference the code snippet, can we embed it in a figure?}.
The scheme object \texttt{fbs} is specialized to sparse logistic regression by specifying its type as \texttt{ForwardBackwardSplitting<forward\_grad\_for\_log\_loss<SpMat>, prox\_l1>}.

We provide implemenations of the following schemes:  Proximal-Point Method,  Gradient Descent, Forward-Backward Splitting, Backward-Forward Splitting, Peaceman-Rachford Splitting, and Douglas-Rachford Splitting.

If the provided schemes are not sufficient, the user may implement their own scheme following certain rules so that their scheme can interact with the rest of the package; see Section \ref{sc:implement}.
The user is encouraged to use objects from the Operator Layer as building blocks, but direct calling the Linear Algebra Layer is perfectly functional. %Their are certain rules about implementing schemes that must be followed; see Section \ref{sc:implement}.

\subsection{Kernel}
A Kernel is a function that an agent executes.
%Agents, in our Toolbox, are realized as threads.
%C++11 threads are created from functions.
%The Kernel Layer contains the functions used to create agents.

As can be seen in Algorithm \ref{alg:fbs_l1_log}, an agent contains a coordinate choice rule and a scheme object.
The agent chooses a coordinate using its rule and call the Scheme object with the chosen coordinate to update $x$.
For each coordinate choice rule, we have implemented a corresponding Kernel function.

We provide the following coordinate choice rules: cyclic, random, and parallel Gauss-Seidel. The cyclic rule picks $i=(i_\mathrm{last} \mod n)+1$. \commwy{explain the rules.}
%block cyclic, and randomized block.
If the user desires a different coordinate rule, they may implement their own Kernel function, following the specifications in Section \ref{sc:implement}.


\subsection{Multicore driver}

%Consider algorithm \ref{alg:fbs_l1_log}.
Once the schemes and kernels have been specified, agents are responsible for carrying them out. Agents are realized as C++11 threads.
%However, agents must be created.
A Multicore Driver creates and manages such agents.
For example, if the user chooses the randomized block coordinate Kernel and ten agents, the Multicore Driver will create ten agents using that kernel. % using the user's specified scheme and the function in the Kernel layer corresponding to the randomized block coordinate rule.
The Multicore Driver is called with a Params object that contains parameters such as kernel choice, number of iterations, and step size.
An example of this is found in code snippet, where \texttt{fbs} and \texttt{params} are passed to the \texttt{MOTAC} driver.

Optionally, a Multicore Driver can launch a controller agent to control the other agents, for example, by choosing step sizes to accelerate convergence.
The current controller agent monitors convergence by periodically computing the fixed point residual.

Most users can treat this layer as a black box. Only when a new way to generate agents is desired, does the user need to modify this layer.

\section{Implementation Details}\label{sc:implement}

\subsection{Interaction between Layers}

The Operator, Scheme, and Kernel Layers interact heavily with each other. To formalize interaction between each layer, we introduce Layer Interfaces. A Layer Interface describes guaranteed member functions of objects in the Layer so that other Layers can safely use these member functions to interact. The Layer Interfaces allow for specialization of objects within each layer while still maintaining a uniform means of interaction. Consider the Operator Interface:
\begin{lstlisting}[language=C++,label={Operator_Interface}]
struct Operator_Interface {

  // returns the operator evaluated on v at the given index
  double operator() (Vector* v, int index);
  // returns the operator evaluated on val at the given index
  double operator() (double val, int index);
  // applies full operator to v_in and write it to v_out
  void operator() (Vector* v_in, Vector* v_out);
  // optional: see CFU paper
  void update_cache_vars (double old_x_i, double new_x_i, int index);
  // update the step size
  void update_step_size (double step_size_) ;
 };
\end{lstlisting}

For an object to belong to the Operator layer, it must have these functions defined.
Attempting to use an object as an Operator that does not have the functionality defined by the Operator Interface will result in compiler errors.

The Scheme Interface:
\begin{lstlisting}[language=C++]
struct Scheme_Interface {
  // update internal params
  void update_params(Params* params);
  // produce coordinate update associated with index
  double operator() (int index);
};
\end{lstlisting}
The Scheme Interface is very lightweight, as the update equations of optimization methods come in a variety of forms. All it requires is that a coordinate update can be produced and that used defined parameters can be passed to the object.
{{\color{red} need to add requirements derived from synchronous driver to Scheme Interface}


\subsection{Templating}

In C++, when objects have similar structures that only vary based upon an input type, templates are used to reduce code redundancy. For instance, the code for an object representing a dense matrix of doubles and the code for an object representing a dense matrix of floats is identical.
Templates are not objects, but instead are blueprints for constructing an object.
Based upon the arguments to the template, a corresponding type is automically constructed. see here *INSERT BRENT* for a primer on templates.
We use templating heavily in our Toolbox, as most of our workflow can be genericized.

Objects in the Scheme Layer are implemented as templates.
This is a natural choice, as object in the Scheme Layer map from optimization method's update to a specific optimization problem.
For example, in code snippet \ref{fbs_l1_log_code} the scheme type of \texttt{fbs} is defined by the arguments to the forward backward splitting template, \texttt{forward\_grad\_for\_log\_loss<SpMat>}, and \texttt{prox\_l1}.

Objects in the Object Layer are also templatized. Depending on data representation, it can be more efficient to use functionality designed for that data representation. Consider the difference between computing $x - \eta \, \nabla_x \,(\sum_{i = 1}^m \log (1 + \exp(-b_i \cdot a_i^T x)))$ when $a_i$ are sparse instead of dense.
Our linear algebra functions are overloaded so the compiler will deduce the proper function to use in the template.

Kernel functions are also templatized. This allows Kernel functions to take in as input arbitrary objects from the Scheme Layer. If we did not use templating, for each coordinate rule would need a function for every possible realization of gradient descent, proximal point method, etc.  Any object that satisfies the Scheme Interface can be passed to a Kernel function.

Multicore Drivers are templatized for the same reason as Kernel functions are templated. If we did not use templating, we would need a Multicore Driver for every possible realization of gradient descent, proximal point method, etc. Any object that satisfies the Scheme Interface can be passed to a Mutlicore Driver.

\subsection{How to write a kernel}
....