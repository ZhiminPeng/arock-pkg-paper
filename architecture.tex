% !TEX root = ./arock_pkg_main.tex
\section{Architecture}


Writing an efficient code is very different from writing an optimization algorithm.
Our toolbox's architecture is designed to mimic how a scientist writes down an optimization algorithm.
The toolbox achieves this by separating into the following layers: Numerical linear algebra, Operator, Scheme, Kernel, and Driver.
Each layer represents a different mathematical component of an optimization algorithm.


The following is a brief description of each layer and how it interacts with the layers above and below it. 

\subsection{Numerical linear algebra}

We use Eigen~\citep{eigenweb}, Sparse BLAS\footnote{\url{http://math.nist.gov/spblas/}}, and BLAS\footnote{\url{http://www.netlib.org/blas/}} in our Toolbox.
Directly using efficient numerical packages like BLAS can be intimidating due to their complex APIs\footnote{} . We provide simplified APIs for common linear algebra operations like $a_i^T x$ in Algorithm \ref{alg:fbs_l1_log}. This layer insulates the user from the grit of raw numerical implementation. Higher layers use the Numerical linear algebra layer in their implementations.
If our provided functions are not sufficient, Eigen, Sparse BLAS, and BLAS are well documented. 

\subsection{Operator}

The Operator Layer contains Forward Operator objects\footnote{``Operator object'' refers to a C++ object. An ``operator'' refers to the mathematical object.} (e.g., gradient descent step, subgradient step) and Backward Operator (e.g., proximal mapping, projection) objects. These operators types see heavy reuse throughtout optimization.
For instance, Algorithm \ref{alg:fbs_l1_log}, along with any other gradient based method for sparse logistic regression, requires the computation of the Forward Operator $x - \eta \, \nabla_x \,(\sum_{i = 1}^m \log (1 + e^{-b_i \cdot a_i^T x}))$. On a similar vein, Nonnegative Matrix Factorization and Nonnegative Least Squares share a backwad operator, the projection onto the positive orthant.

Much as the Numerical Linear Algebra Layer insulates the user from the computation details of numerical linear algebra, the Operator Layer insulates the user from the computational details of operators. This is achieved by encapsulating the computation of common forward and backward operators into Operator objects. Abstracting operators into Operator objects is useful, as Operator objects provide clarity and modularity. Consider the construction of  \texttt{forward} in Listing \ref{code:l1log}. At a glance, it is clear that we are using the gradient of logistic regression, the data is sparse, and we are using the Matrix \texttt{A} and Vector \texttt{b} to compute the gradient. Listing \ref{code:l1log} and Listing \ref{code:LASSO} demonstrates how Operator objects provide modularity. Perturbations to an algorithm, such as a change of regularizer or a change of data fidelity term, can be handled by changing the corresponding operator type. The rest of the code structure is unchanged. The Operator Layer, as a result, allows users to reason and code at the level of operators. 
The higher layers use Operator objects as components in the creation of algorithms.
 
As our Toolbox is designed for coordinate update methods, each operator is implemented to compute coordinates efficiently.
 A coordinate or block of coordinates can be computed efficienty if the computational cost of a single coordinate or a block of coordinates of the operator is reduced by a dimensional factor compared to the evaluation of the entire operator (e.g. $(Ax)_i$ versus $Ax$). 
 In some cases caching, the storing an intermediate computations, can improve the efficiency of updating a coordinate block. These ideas are formalized in the paper on coordinate friendly stuctures~\citep{PengWuXuYanYin2016_coordinate}.

In the case that an operator is needed that we have not provided, a user can use~\citep{PengWuXuYanYin2016_coordinate} as a guide to identify coordinate-friendly structures and implement their own operator. In addition, there are certain rules about operator implementation that must be followed; see Section \ref{sc:interface}.


\subsection{Scheme}
A scheme describes how to make a single-iteration update to $x$.
It can be written as a combination of operators. For example, \eqref{eq:fbs_l1_log} is the Forward-Backward Splitting Scheme (also referred as Proximal Gradient Method) for a specific problem, sparse logistic regression. In Algorithm \ref{alg:fbs_l1_log}, it corresponds to Line 9. To apply the Forward-Backward Splitting Scheme to the sparse logistic regression problem \ref{code:tikhonovlog} , we need to specify a forward operator and a backward operator (e.g., on Line 5 of Algorithm \ref{alg:fbs_l1_log}). We can see this in Listing \ref{code:l1log}.
The scheme object \texttt{fbs} is specialized to sparse logistic regression by specifying its type as \texttt{ForwardBackwardSplitting<forward\_grad\_for\_log\_loss<SpMat>, prox\_l1>}.

We provide implemenations of the following schemes:  Proximal-Point Method,  Gradient Descent, Forward-Backward Splitting, Backward-Forward Splitting, Peaceman-Rachford Splitting, and Douglas-Rachford Splitting.

If the provided schemes are not sufficient, the user may implement their own scheme following certain rules so that their scheme can interact with the rest of the package; see Section \ref{sc:interface}.
The user is encouraged to use objects from the Operator Layer as building blocks, but direct calling the Numerical Linear Algebra Layer is perfectly functional. %Their are certain rules about implementing schemes that must be followed; see Section \ref{sc:implement}.

\subsection{Kernel}
A Kernel function described the operations that an agent performs.
As can be seen in Algorithm \ref{alg:fbs_l1_log}, it corresponds to the while-loop, which contains contains a coordinate choice rule and a scheme object.
The agent chooses a coordinate using its rule and call the Scheme object with the chosen coordinate to update $x$.
For each coordinate choice rule, we have implemented a corresponding Kernel function.

We provide the following coordinate choice rules: cyclic, random, and parallel Gauss-Seidel. 
Cyclic update rule divides coordinates into approximately equal-sized blocks, and each agent is assigned a block.
Each agent chooses coordinates cyclicly from within the block. 
For random update rule, each agent randomly chooses a block, then chooses coordinates cyclicly from within that block.
For parallel Gauss-Seidel rule, each agent updates all of the coordinates in a Gauss-Seidel fashion. 
If the user desires a different coordinate rule, they may implement their own Kernel function, following the specifications in Section~\ref{sc:implement}.


\subsection{Driver}

Once the schemes and kernels have been specified, agents are responsible for carrying them out. Agents are generated as C++11 threads.
A Driver creates and manages such agents.
For example, if the user chooses the cyclic coordinate Kernel and ten agents, the Multicore Driver will create ten agents using that kernel. % using the user's specified scheme and the function in the Kernel layer corresponding to the randomized block coordinate rule.
The Multicore Driver is called with a Params object that contains parameters such as kernel choice, number of iterations, and step size.
An example of this is found in Listing \ref{code:l1log}, where \texttt{fbs} and \texttt{params} are passed to the \texttt{MOTAC} driver.

Optionally, a Multicore Driver can launch a controller agent to control the other agents, for example, by dynamically updating step sizes to accelerate convergence.
The current controller agent monitors convergence by periodically computing the fixed point residual, a measure that would reduce to 0 when the sequence converges to a fixed point.

Most users can treat this layer as a black box. Only when a new way to generate agents is desired, does the user need to modify this layer.

\section{Implementation details}\label{sc:implement}

\subsection{Interaction between layers} \label{sc:interface}

The Operator, Scheme, and Kernel Layers interact heavily with each other. To formalize interaction between each layer, we introduce Layer Interfaces. A Layer Interface describes guaranteed member functions of objects in a Layer so that other Layers can safely use these member functions to interact. The Layer Interfaces allow for specialization of objects within each layer while still maintaining a uniform means of interaction. Consider the Operator Interface:
\begin{lstlisting}[language=C++,label=Operator_Interface]
class OperatorInterface {
public:
  // compute operator at index
  virtual double operator() (Vector* v, int index = 0)=0;
  // compute operator using val at index
  virtual double operator() (double val, int index = 0)=0;
  // compute full operator using v_in, storing in v_out
  virtual void operator() (Vector* v_in, Vector* v_out)=0;
  // update operator related step size
  virtual void update_step_size(double step_size_)=0;
  // update cache variable following an update at index i
  virtual void update_cache_vars(double old_x_i, double new_x_i, int i)=0;
  // update block of cache variables based upon rank of calling thread
  virtual void update_cache_vars(Vector* x, int rank, int num_threads)=0;
};
\end{lstlisting}

For an object to belong to the Operator layer, it must inherit from the Operator Interface.
Code that contains an object that inherits from the Operator Interface that does not implement the functions in the Operator Interface will not compile.

The Scheme Interface:
\begin{lstlisting}[language=C++]
class SchemeInterface {
public:
  // update scheme internal parameters
  virtual void update_params(Params* params)=0;
  // async: compute and apply coordinate update, return S_{index}
  virtual double operator() (int index)=0;
  // sync: compute and store S_{index} in S_i
  virtual void operator() (int index, double& S_i)=0;
  // sync: apply block of S stored in s to solution variable
  virtual void update(Vector& s, int range_start, int num_cords)=0;
  // sync: apply coordinate of S stored in s to solution variable
  virtual void update(double s, int idx)=0;
  // sync: update rank worth of cache_vars based on num_threads
  virtual void update_cache_vars(int rank, int num_threads)=0;
};
\end{lstlisting}
For an object to belong to the Scheme Layer, it must inherit from the Scheme Interface.
Code that contains an object that inherits from the Scheme Interface that does not implement the functions in the Scheme Interface will not compile.

Schemes must be suitable for both asynchronous and synchronous computing. Synchronous computing requires all coordinate updates to be computed before applying the coordinate updates. Asynchronous computing can apply coordinates updates immediately. The Scheme Interface reflects these two computing regimes.

The Scheme Interface is very lightweight, as the iterations of optimization methods come in a variety of forms.  All it requires is that a coordinate update can be produced and that parameters can be passed to the object.

 


\subsection{Kernel and Multicore Driver Interaction}

A Multicore Driver creates agents using Kernel functions. The arguments to a Multicore Driver are a scheme object and a params object. A new Kernel function must be callable using information from those two objects. The Multicore Drivers must be modified to include the new kernel function as an option for creating an agent. The modfication requires adding a new case to an if-elseif chain, and knowledge of how to create a C++11 thread.

\subsection{Templating}

In C++, when objects have similar structures that only vary based upon an input type, templates are used to reduce code redundancy. For instance, the code for an object representing a dense matrix of \emph{doubles} and the code for an object representing a dense matrix of \emph{floats} is identical.
Templates are not objects, but instead are blueprints for constructing an object.
Based upon the arguments to the template, a corresponding type is automically constructed.
We use templating heavily in our Toolbox, as most of our workflow can be genericized.

Objects in the Scheme Layer are implemented as templates.
This is a natural choice, as the Scheme objects are generic iteration formulas.
For example, in Listing \ref{code:l1log}, the scheme type of \texttt{fbs} is defined by the arguments to the forward backward splitting template, \texttt{forward\_grad\_for\_log\_loss<SpMat>}, and \texttt{prox\_l1}.

Objects in the Object Layer are also templatized. Depending on data representation, it can be more efficient to use functionality designed for that data representation. Consider the difference between computing $x - \eta \, \nabla_x \,(\sum_{i = 1}^m \log (1 + e^{-b_i \cdot a_i^T x}))$ when $a_i$ is stored in the  sparse format\footnote{Only the values and locations of  nonzero entries are stored.} instead of the dense format.
Our linear algebra functions are overloaded so the compiler will deduce the proper function to use in the template.

Kernel functions are also templatized. This allows Kernel functions to take in as input arbitrary objects from the Scheme Layer. If we did not use templating, for each coordinate rule would need a function for every possible realization of gradient descent, proximal point method, etc.  Any object that satisfies the Scheme Interface can be passed to a Kernel function.

Multicore Drivers are templatized for the same reason as Kernel functions are templated. If we did not use templating, we would need a Multicore Driver for every possible realization of gradient descent, proximal point method, etc. Any object that satisfies the Scheme Interface can be passed to a Mutlicore Driver.
