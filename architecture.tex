% !TEX root = ./arock_pkg_main.tex
\section{Architecture}

Writing a compute code is very different from writing down an optimization algorithm on paper.
Our toolbox's architecture is designed to mimic how a scientist writes down an optimization algorithm.
The toolbox achieves this by separating into the following layers: Linear Algebra, Operators, Schemes, Kernels, and Multicore Drivers.
Each layer represents a different mathematical component of a multicore optimization method. 

The following is a brief description of each layer and how it interacts with the layers above and below it.

\subsection{Numerical Linear Algebra}

We use eigen, Sparse BLAS, and BLAS in our Toolbox for basic 
numerical linear algebra tasks such as dot products, norm calculation, matrix vector multiplication.
For example, they implement $a_i^T x$ in Algorithm 1. This layer is called by the functions and operators in the higher layers. This layer is introduced to insulate the user from the grit of raw numerical implementation. 

\subsection{Operators}

Many optimization methods have similar components.
For instance, both LASSO and Nonnegative Least Squares requires the computation of the gradient of $||Ax-b||_2^2$.
Sparse Logistic Regression and LASSO share the evaluation of a shrinkage operator.
On a similar vein, Nonnegative Matrix Factorization and Nonnegative Least Squares share a projection operator.
The Operator layer formalizes these components as Operator objects.
The computations necessary to compute a reusable component is wrapped in an Operator object.


As our Toolbox is designed for coordinate update methods, each Operator is implemented to compute coordinates efficiently. For example, \texttt{forward\_grad\_for\_log\_loss<SpMat>} requires the matrix \texttt{A}, the vector \texttt{b}, a stepsize \texttt{eta},  and, to maintain computational efficiency, the cached variable \texttt{Atx}.
In some cases the data may be dense, sparse, or best represented by a function (consider the wavelet tranform).
Operators, to allow for different data respresentations, are templatized on their data.
The matrix \texttt{A}, in this example, is sparse.

By introducing Operator objects, higher layers are insulated from the implementation details of an Operator. 
However, higher layers still need to interact with objects in the Operator Layer in a friendly and uniform way.
Our toolbox provides a uniform means of interacting with Operator objects through the Operator Interface.
Every Operator object is guaranteed to have the following functions defined:

*insert here*

\subsection{Schemes}

The Scheme layer is intended to model the divide between an optimization method and an algorithm. 
For example, equation \ref{eq:fbs_l1_log} is Forward Backward Splitting (also referred as Proximal Gradient Method) applied to a specific problem, sparse logistic regression.
 A realization of  Forward Backward Splitting is constructed by composing a specific Forward Operator and a specific Backward Operator.
 In this case, a gradient step of the logistic loss and the shrinkage operator. 
At a high level, the scheme layer implements optimization methods as algorithm factories.
These factories map Operators objects and a optimization method to an algorithm.  
We provide implemenations of the following schemes: \texttt{ForwardBackwardSplitting}, \texttt{ProximalPointAlgorithm},  \texttt{GradientDescent}, \texttt{BackwardForwardSplitting}, and \texttt{PeacemanRachfordSplitting}.

Objects in the Scheme Layer are implemented as templates.
Templates, in c++, are code factories.
Based upon the arguments passed to the template, c++ automatically constructs a corresponding object type for the user.
For example, in code snippet \ref{fbs_l1_log_code} the scheme type of \texttt{fbs} is defined by the arguments to the forward backward splitting template, \texttt{forward\_grad\_for\_log\_loss<SpMat>}, and \texttt{prox\_l1}. 
Different arguments to the template result in different coordinate update schemes.

A user can construct solvers by choosing a scheme, and selecting operator objects to specialize the scheme to a specific problem. 
If the provided schemes are not sufficient, the user may implement their own scheme.
The user is encouraged to use objects from the Operator Layer as building blocks, but direct implementaiton at the level of Linear Algebra is perfectly functional.


Objects in the Scheme Layer are cooridante update methods.
Schemes are not yet, however, solvers.
Higher layers need to interact with objects in the Scheme Layer to construct solvers.
Our toolbox provides a uniform means of interacting with Scheme objects through the Scheme Interface.
Every Scheme object is guaranteed to have the following functions defined:

\begin{lstlisting}[language=C++]
struct Scheme_Interface {
  void update_params(Params* params);
  double operator() (int index);
};
\end{lstlisting}

\subsection{Kernel}

A solver is comprised of a coordinate choice rule, a coordinate update scheme, and a loop of execution.
For each coordinate rule, there is a corresponding working function in the Kernel Layer.

The Multicore Drivers Layer is responsible for creating agents, and the Kernel layer contains the agents types that can be created.
A worker agent chooses an index according to its rule, and, using a scheme object, computes a coordinate update.
As such, .
In addition, the Kernel layer contains controller agents.
A controller agent manages the worker agents by choosing stepsizes to accelerate convergence.
The current controller agent monitors convergence by maintaining an approximate fixed point residual.

Agents are realized as c++11 threads and a c++11 thread is constructed from a function.
As a result, each worker and controller type, has  a corresponding function.
If the user wishes to introduce a new agent type, they need only write a function that implements the logic of the new agent.
They must then modify the Multicore Driver layer to recognize their new agent type as an option.




\subsection{Multicore Drivers}


Consider algorithm \ref{alg:fbs_l1_log}.
 The algorithm assumes the existence of computing agents.
The Multicore Drivers  layer is responsible for the creation of agents.
The  algorithm \ref{alg:fbs_l1_log} provides a concrete recipe for creating an agent: a coordinate update scheme, a coordinate choice rule, a stepsize, and an iteration count.
At a high level, this section can be understood as a mapping from a coordinate update scheme to a multicore solver.

As seen in Section~\ref{sec:quick_start}, the \pkg~driver, takes as input a scheme object, and a parameters object.
The parameters object determines how coordinates are chosen, how many iterations to run, and the stepsize.
Coordinate choices include cyclic, block cyclic, and randomized block. 
\pkg~launches creates  worker agents, agents that interact with the solver object to produce coordinate updates, and a controller agent, an agent that manages the worker threads.
The number of worker agents is specified through the parameters object.
Agents are realized as c++11 threads.

Most users can treat the Multicore Drivers as black box functionality.
As the details of a coordinate update scheme are packaged into a scheme object, our Multicore Drivers are insulated from most common code modficiations.
In the case that a different coordinate rule or a specialization of work distribution is desired, the Multicore Driver layer must be understood at a lower level.
To understand  Multicore Drivers later at a lower level, it is necessary to know how to launch and join threads.

To maintain the mathematical abstraction between layers, Interfaces are used when necessary. 
Interfaces are a set of assumed functionality of a layer.
Layers interact through their interfaces.
Consider the Interface for the Operator object:

\begin{lstlisting}[language=C++,label={Operator_Interface}]
struct Operator_Interface {
  // returns the operator evaluated on v at the given index
  double operator() (Vector* v, int index);
  // returns the operator evaluated on v at the given index
  double operator() (double val, int index);
  //applies full operator to v_in and write it to v_out
  void operator() (Vector* v_in, Vector* v_out);
  //optional: see CFU paper
  void update_cache_vars (double old_x_i, double new_x_i, int index);
  // update the step size
  void update_step_size (double step_size_) ;
 };
\end{lstlisting}

For an object to belong to the Operator layer, it must have these functions defined.
Attempting to use an object as an Operator that does not have the functionality defined by the Operator Interface will result in compiler errors.
Note that the Operator Inteface only includes the function declarations, not the implementation of the functions themselves.
As a result, implementation is decoupled from usage.
This allows for easy maintenance and specialization of code while not affecting the user's code.

