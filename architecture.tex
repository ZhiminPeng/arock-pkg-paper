% !TEX root = ./arock_pkg_main.tex
\section{Architecture}

The toolbox can be broken into the following layers: Multicore Drivers, Kernels, Schemes, Operators, and Linear Algebra.
Each layer represents a different mathematical component. 
Object in each layer interact through as set of assumed functionality, their interface.
Consider the Interface for the Operator object:

\begin{lstlisting}[language=C++,label={Operator_Interface}]
struct Operator_Interface {
   // returns the operator evaluated on v at the given index
   double operator() (Vector* v, int index);
  // returns the operator evaluated on v at the given index
   double operator() (double val, int index);
  //applies full operator to v_in and write it to v_out
   void operator() (Vector* v_in, Vector* v_out);
  //optional: see CFU paper
  void update_cache_vars (double old_x_i, double new_x_i, int index);
  // update the step size
   void update_step_size (double step_size_) ;
 };
\end{lstlisting}

For an object to belong to the Operator layer, it must have these functions and constructors defined.
Attempting to use an object as an Operator that does not have the functionality defined by the Operator Interface will result in compiler errors.
Note that the Operator Intefarce only includes the function declarations, not the implementation of the functions themselves.

The following is a brief description of each layer and how it interacts with the layers above and below it.

\subsection{Multicore Drivers}

As seen in Section~\ref{sec:quick_start}, the \pkg~driver, takes as input, a scheme object, and a parameters object.
The parameter object determines how coordinate are chosen, how many iterations to run the solver, and the stepsizes associated with the solver.
Coordinate choices include cyclic, block cyclic, and randomized block.
The parameter object also determines the multicore functionality. 
Based upon the parameter object, \pkg~launches worker threads, threads that interact with the solver object to produce coordinate updates, and a controller thread, a thread that manages the worker threads.
  

\subsection{Kernel}

For each coordinate choice rule, there is a corresponding worker thread.
A worker thread iteratively chooses an index according to its rule, and, using a scheme object, computes a coordinate update.
The controller thread manages the worker threads by choosing stepsizes to accelerate convergence.
The controller thread monitors convergence by maintaining an approximate fixed point residual.

\subsection{Schemes}
Schemes are objects with, at minimum, member functions as defined by the Scheme Interface.

\begin{lstlisting}[language=C++]
struct Scheme_Interface {
void update_params(Params* params);
double operator() (int index);
};
\end{lstlisting}
By providing a coordinate to the scheme object, a coordinate update is applied to the solution variable.
 The Scheme Interface is very lightweight, as schemes can be quite varied in nature.
 We provide implemenations of the following schemes: \texttt{ForwardBackwardSplitting}, \texttt{ProximalPointAlgorithm},  \texttt{GradientDescent}, \texttt{BackwardForwardSplitting}, and \hfill \break      \texttt{PeacemanRachfordSplitting}.
Mathematically, Schemes are recipes for coordinate update.
To provide similar code functionality, schemes are templatized.  
For example, in \ref{fbs_l1_log_code} the type of \texttt{fbs} is defined by the arguments to the forward backward splitting template, \texttt{forward\_grad\_for\_log\_loss<SpMat>}, and \texttt{prox\_l1}. 
Different arguments to the template result in different versions of the scheme. 

\subsection{Operators}

Operators are objects with, at minimum, member functions as defined by \ref{Operator_Interface}, the Operator Interface.
The construction of an Operator object depends upon what is necessary to compute its coordinate update efficiently.
For example, \texttt{forward\_grad\_for\_log\_loss<SpMat>} requires the matrix \texttt{A}, the vector \texttt{b}, a stepsize \texttt{eta},  and, to maintain computational efficiency, the cached variable \texttt{Atx}.
In some cases the data may be dense, sparse, or best represented by a function (consider the wavelet tranform).
Operators, to allow for different data respresentations, are templatized on their data.
The matrix \texttt{A}, in this example, is sparse.




\subsection{Linear Algebra}

We use Eigen package to represent sparse matrices and vectors. 
Sparse linear algebra is provided through an interface to Eigen and Sparse BLAS.
Dense linear algebra is provided through an interface to BLAS. 







